\documentclass{article}
\usepackage{amsmath}
\usepackage{enumerate}
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\pd}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\var}{\mathop{\mathrm{Var}}}

\title{Statistics 221 Final Project: C++ team}
\begin{document}
\maketitle

\section{Compare MLE to SGD}
MLE methods such Newton-Raphson compute the gradient of the log-likelihood, $\nabla\ell$ for all observations in one step. While this works well for small samples, it is too computationally expensive to be done with especially large amounts of data. Sakrison's method relies on the assumption that the expectation of the gradient for a single observation is proportional to the gradient for all observations. By the law of large numbers, repeatedly updating using the gradient for single observations will converge to expectations. This makes it possible to do the update step for a single observation at a time and compute the MLE.

The implicit updates use the update step, $\vec{\theta}_{t+1} = \vec{\theta}_t + a_t\nabla\ell(\vec{\theta}_{t+1}; y_t, \vec{x}_t)$ instead of $\vec{\theta}_{t+1} = \vec{\theta}_t + a_t\nabla\ell(\vec{\theta}_{t}; y_t, \vec{x}_t)$. However the expectation of the gradient is the same for all observations. Therefore $\nabla\ell(\vec{\theta}_{t+1}; y_t, \vec{x}_t) = \nabla\ell(\vec{\theta}_{t}; y_t, \vec{x}_t)$. Since the implicit method has the same expectation as Sakrison's method, it must also compute the MLE.


\section{GLM Proofs}

\begin{enumerate}[(a)]
\item Show $E(y_t|\vec{x}_t) = h(\vec{x}_t^T\vec{\theta}^*) = b'(\eta_t)$ \\
We start with the moment generating function, which we solve for using LOTUS
\begin{align*}
M_Y(t) &= E[e^{tY}]\\
&= \int_Y \exp(ty_k)f(y_k|\eta_k)dy_k\\
&= \int_Y \exp(ty_k)\exp\bigg(\frac{\eta_ty_t - b(\eta_t)}{\phi}\bigg)\cdot c(y_t, \phi)dy_k\\
&= \int_Y c(y_t, \phi)\exp\bigg(ty_k + \frac{\eta_ty_t}{\phi} - \frac{b(\eta_t)}{\phi}\bigg) dy_k\\
&= \int_Y c(y_t, \phi)\exp\bigg(\bigg(\frac{\eta_ty_t + \phi t}{\phi}\bigg)y_k - \frac{b(\eta_t)}{\phi}\bigg) dy_k\\
&= \int_Y c(y_t, \phi)\exp\bigg(\bigg(\frac{\eta_ty_t + \phi t}{\phi}\bigg)y_k - \frac{b(\eta_k + \phi t)}{\phi} + \frac{b(\eta_k + \phi t)}{\phi} - \frac{b(\eta_t)}{\phi}\bigg) dy_k\\
&= \int_Y \exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)c(y_t, \phi)\exp\bigg(\frac{(\eta_ty_t + \phi t) y_k - b(\eta_k + \phi t)}{\phi}\bigg) dy_k\\
&= \exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\int_Y c(y_t, \phi)\exp\bigg(\frac{(\eta_ty_t + \phi t) y_k - b(\eta_k + \phi t)}{\phi}\bigg) dy_k\\
\end{align*}
The remaining integral is the conditional pdf of $y|\eta_k + \phi t$ and integrates to 1, which leaves the remaining MGF for $Y$
$$M_Y(t) = \exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg) $$
We then use the MGF to find $E(y_t|\vec{x}_t)$. We start by finding $M'_Y(t)$
\begin{align*}
M'_Y(t) &= \pd{}{t} \bigg[\exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\bigg]\\
&= \exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\cdot\frac{b'(\eta_k + \phi t)\phi}{\phi}\\
&= \exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\cdot b'(\eta_k + \phi t)\\
\end{align*}
We then evaluate at $t=0$ which is the first moment.
\begin{align*}
E(y_t|\vec{x}_t) &= M'_Y(0)\\
&= \exp\bigg(\frac{b(\eta_k + \phi\cdot 0) - b(\eta_t)}{\phi}\bigg)\cdot b'(\eta_k + \phi\cdot 0)\\
&= \exp\bigg(\frac{0}{\phi}\bigg)\cdot b'(\eta_k)\\
&= b'(\eta_k)\\
\end{align*}

\item Show $\var(y_t|\eta_t) = \phi\cdot h'(\eta_t)$\\
We find the variance using the moment generating function from part (a).
$$\var(y_k|\eta_k) = E(y^2_k|\eta_k) - [E(y_k|\eta_k)]^2$$
We can use the value of $E(y_k|\eta_k)$ from part (a). We then find the second moment $E(y^2_k|\eta_k)$ with our MGF.
\begin{align*}
M''_Y(t) =& \pd{}{t} \bigg[\exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\cdot b'(\eta_k + \phi t)\bigg]\\
=& \exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\cdot b'(\eta_k + \phi t)\cdot b'(\eta_k + \phi t) +\\
& b''(\eta_k + \phi t)\cdot\phi\cdot\exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\\
=& \exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\cdot \bigg[b'(\eta_k + \phi t)\bigg]^2 + b''(\eta_k + \phi t)\cdot\phi\cdot\exp\bigg(\frac{b(\eta_k + \phi t) - b(\eta_t)}{\phi}\bigg)\\
\end{align*}
Evaluating at 0 gives
\begin{align*}
 E(y^2_k|\eta_k) =& M''_Y(0)\\
=& \exp\bigg(\frac{b(\eta_k + \phi *0) - b(\eta_t)}{\phi}\bigg)\bigg[b'(\eta_k + \phi *0)\bigg]^2 + b''(\eta_k + \phi *0)\phi\exp\bigg(\frac{b(\eta_k + \phi *0) - b(\eta_t)}{\phi}\bigg)\\
=& \exp(\frac{0}{\phi})\bigg[b'(\eta_k)\bigg]^2 + \phi \cdot b''(\eta_k)\exp(\frac{0}{\phi})\\
=& \bigg[b'(\eta_k)\bigg]^2 + \phi \cdot b''(\eta_k)\\
\end{align*}
Putting the two parts together gives the solution
\begin{align*}
\var(y_t|\eta_t) &= E(y^2_k|\eta_k) -  [E(y_k|\eta_k)]^2\\
&= [b'(\eta_k)]^2 + \phi \cdot b''(\eta_k) -  [b'(\eta_k)]^2\\
&= \phi \cdot b''(\eta_k)\\
\end{align*}
From part (a) we know that $b'(\eta_t) = h(\eta)$ therefore $b''(\eta_t) = h'(\eta_t)$ which gives
$$\var(y_t|\eta_t) = \phi\cdot h'(\eta_t)$$

\item Show $\nabla\ell(\vec{\theta}; y_t, \vec{x}_t) = \frac{1}{\phi}(y_t - h(\vec{x}_t^T\theta))\vec{x}_t$\\
To find $\nabla\ell(\vec{\theta}; y_t, \vec{x}_t)$, we take the partial derivative with respect to $\vec{\theta}$. The likelihood is proportional to the pdf.
\begin{align*}
L(\vec{\theta}; y_t, \vec{x}_t) &\propto \exp\bigg(\frac{\eta_ty_t - b(\eta_t)}{\phi}\bigg)\cdot c(y_t, \phi)\\
\ell(\vec{\theta}; y_t, \vec{x}_t) &=  \frac{\eta_ty_t - b(\eta_t)}{\phi}\\
\nabla\ell(\vec{\theta}; y_t, \vec{x}_t) &= \pd{}{\vec{\theta}}\bigg[\ell(\vec{\theta}; y_t, \vec{x}_t)\bigg]\\
&= \pd{}{\vec{\theta}}\bigg[\frac{\eta_ty_t - b(\eta_t)}{\phi}\bigg]\\
&= \pd{}{\vec{\theta}}\bigg[\frac{\vec{x}_t^T\vec{\theta}y_t - b(\vec{x}_t^T\vec{\theta})}{\phi}\bigg]\\
&= \frac{1}{\phi}(\vec{x}_ty_t - \vec{x}_tb'(\vec{x}_t^T\vec{\theta}))\\
&= \frac{1}{\phi}(y_t - h(\vec{x}_t^T\vec{\theta}))\vec{x}_t\\
\end{align*}

\item Show $\mathcal{J}(\vec{\theta}) = -E(\nabla\nabla\ell(\vec{\theta}; y_t, \vec{x}_t)) = \frac{1}{\phi}E(h'(\vec{x}_t^T\vec{\theta})\vec{x}_t\vec{x}_t')$\\
To find the Fisher information, we take the negative expectation of the second partial derivative of the log-likelihood with respect to $\vec{\theta}$. We start with the the solution from part (c).
\begin{align*}
\mathcal{J}(\vec{\theta}) &= -E\bigg(\nabla\nabla\ell(\vec{\theta}; y_t, \vec{x}_t)\bigg)\\
&= -E\bigg(\pd{}{\vec{\theta}}\bigg[\frac{1}{\phi}(y_t - h(\vec{x}_t^T\vec{\theta}))\vec{x}_t\bigg]\bigg)\\
&= -E\bigg(-\frac{1}{\phi}\cdot h'(\vec{x}_t^T\vec{\theta})\vec{x}_t\vec{x}_t^T\bigg)\\
&= \frac{1}{\phi}\bigg(h'(\vec{x}_t^T\vec{\theta})\vec{x}_t\vec{x}_t^T\bigg)\\
\end{align*}

\end{enumerate}



\end{document}